Load packages and data
```{r, warning = FALSE, message = FALSE}
library(tidyverse)
library(tidytext)
library(ngram)
library(schrute)
office_ratings <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-17/office_ratings.csv')
office_scripts <- schrute::theoffice
#View(office_ratings)
#View(office_scripts)
```

"That's what she said" by character:
```{r}
# Find line/character combo right before a "TWSS"
# Initialize (might be a bit hacky, but works)
office_scripts$text_before = c()
office_scripts$text_before[1] = NA
office_scripts$text_before[2:nrow(office_scripts)] = office_scripts$text[1:(nrow(office_scripts)-1)]

office_scripts$character_before = c()
office_scripts$character_before[1] = NA
office_scripts$character_before[2:nrow(office_scripts)] = office_scripts$character[1:(nrow(office_scripts)-1)]

# Filter for TWSS lines only 
office_scripts_twss = office_scripts %>%
  filter(str_detect(text, pattern = "That's what she said|that's what she said"))

office_scripts_twss %>%
  count(character, sort = TRUE)
# Might not include ALL occurrences? E.g., if a character said "TWSS" twice in a single line...

#office_scripts_twss = office_scripts %>%
#  filter(str_count(text, pattern = "That's what she said|that's what she said") > 0)
# (We'll go with unique lines for simplicity)

office_scripts_twss %>%
  select(character, text_before) %>% 
  View()

office_scripts_twss %>%
  count(character, sort = TRUE) %>%
  ggplot(aes(x = fct_reorder(character, n), y = n)) + 
  geom_col(fill = "dodgerblue") + 
  labs(x = "", y = "Number of 'That's what she said' lines", 
       title = "The Office: That's what she said!", 
       caption = "Source: schrute R package") + 
  theme_minimal() + 
  theme(axis.text.x = element_text(face = "bold", size = 10), 
        axis.text.y = element_text(face = "bold", size = 10), 
        axis.title.x = element_text(face = "bold", size = 12), 
        axis.title.y = element_text(face = "bold", size = 12), 
        plot.title = element_text(face = "bold", size = 14), 
        plot.caption = element_text(color = "gray")) + 
  coord_flip()
```

It looks like Michael says "That's what she said" the most, *by far*. Let's see what characters trigger this the most...
```{r}
office_scripts_twss_michael = office_scripts_twss %>%
  filter(character == "Michael")

View(office_scripts_twss_michael)

office_scripts_twss_michael %>%
  count(character_before, sort = TRUE) %>%
  ggplot(aes(x = fct_reorder(character_before, n), y = n)) + 
  geom_col(fill = "dodgerblue") +
  ylim(0, 8) +
  geom_label(aes(x = 6, y = 3.75, label = "Michael! You are making this harder than it has to be.")) +
  geom_label(aes(x = 14, y = 5.5, label = "And you were directly under her the entire time?")) +
  geom_label(aes(x = 4, y = 3.17, label = "And... go. Force it in as deep as you can.")) +
  geom_label(aes(x = 2, y = 2.17, label = "You already did me.")) +
  geom_label(aes(x = 7, y = 4.03, label = "Michael. Don't. Don't. Don't make it harder than it has to be.")) + 
  labs(x = "", y = "Number of lines triggering a 'That's what she said'", 
       title = "The Office: Who triggers Michael the most?",
       subtitle = "Lines preceeding Michael saying 'That's what she said'", 
       caption = "Source: schrute R package") + 
  theme_bw() + 
  theme(axis.text.x = element_text(face = "bold", size = 10), 
        axis.text.y = element_text(face = "bold", size = 10), 
        axis.title.x = element_text(face = "bold", size = 12), 
        axis.title.y = element_text(face = "bold", size = 12), 
        plot.title = element_text(face = "bold", size = 14), 
        plot.caption = element_text(color = "gray")) + 
  coord_flip()
```

**Sentiment Analysis***

- Are some words more *positive* or *negative*?

- The *Bing* lexicon contains words, and whether or not that word has a *positive* or *negative* sentiment
```{r}
View(get_sentiments("bing"))
```

In order to judge sentiment of *words*
- Use `unnest_tokens()` to get each *word* on its own line:
- `input` is the name of the *existing* column that I want to unnest
- `output` is the name of the new column (this will contain one word per row)
```{r}
#?unnest_tokens
office_scripts_tidy = unnest_tokens(office_scripts, input = text, output = word)
#View(office_scripts_tidy)
```

Now let's **join** the *Bing* lexicon to the Office script data. 
- Use `inner_join()` to join the Bing lexicon words and the words from the Office
- But only keep the words that appear in *BOTH* tables (e.g., this will remove "stop words" from the Office data)
```{r}
office_scripts_sentiment = office_scripts_tidy %>%
  inner_join(get_sentiments("bing"), by = "word")
#View(office_scripts_sentiment)
```

Let's see which *positive* and *negative* words are used the most frequently. 
```{r}
office_scripts_sentiment %>%
  count(sentiment, word, sort = TRUE)
```

Sentiment "scores" for each episode:
```{r}
office_scripts_sentiment %>%
  group_by(episode_name) %>%
  count(sentiment, sort = TRUE) 


office_scripts_sentiment %>%
  count(episode_name, sentiment) %>%
  spread(sentiment, n) %>%
  group_by(episode_name) %>%
  mutate(sentiment_score = positive - negative, 
         positive_prop = sum(positive)/(sum(positive)+sum(negative))) %>%
  ggplot(aes(x = episode_name, y = positive_prop)) + 
  geom_col() + 
  coord_flip()
```

Uh oh, that's ugly. Let's do something better...
- Look at *distribution* of `positive_prop` *by season*
```{r}
library(ggridges)
office_scripts_sentiment %>%
  count(season, episode, sentiment) %>%
  spread(sentiment, n) %>%
  group_by(season, episode) %>%
  mutate(sentiment_score = positive - negative, 
         positive_prop = sum(positive)/(sum(positive)+sum(negative))) %>%
 ggplot(aes(x = positive_prop, y = factor(season))) + 
  geom_density_ridges() + 
  theme_minimal() + 
  labs(x = "Proportion of positive words (by episode)", y = "Season Number", 
       title = "The Office: Distribution of Positive Sentiment Words")
```



















How many lines does each main character have:
```{r}
office_scripts_main = office_scripts %>%
  filter(character %in% c("Michael", "Dwight", "Jim", "Pam", "Andy"))

#View(office_scripts_main)

office_scripts_main %>%
  count(character, sort = TRUE) %>%
  ggplot(aes(x = character, y = n)) + 
  geom_col() + 
  theme_minimal() + 
  labs(x = "", y = "Number of Lines") +
  scale_y_continuous(label = scales::comma) +
  coord_flip() 
```

Count Number of *WORDS* per character:
```{r}
wordcount(office_scripts$text[1])


office_scripts_season1 = office_scripts %>%
  filter(season == 1) %>%
  group_by(index) %>% # groups by each line
  mutate(word_count = wordcount(text))
  
office_scripts_season1 %>%
  group_by(character) %>%
  summarize(total_words = sum(word_count), 
            mean_words = mean(word_count)) %>%
  #arrange(desc(mean_words))
  top_n(5, wt = mean_words) %>%
  ggplot(aes(x = fct_reorder(character, mean_words), y = mean_words)) + 
  geom_col() + 
  coord_flip() + 
  theme_minimal()
```


Let's look at the phrase "that's what she said":
- Use the `stringr` package functions (part of `tidyverse`)
```{r}
str_which(office_scripts$text, pattern = "that's what she said")

#office_scripts$text[c(4655, 5874)]

str_subset(office_scripts$text, pattern = "That's what she said|that's what she said")
```

See how many times "Michael" is referenced
```{r}
office_scripts %>%
  mutate(michael_ref = 
           str_detect(text, pattern = "Michael|michael")) %>%
  group_by(character) %>%
  summarize(N_michael = sum(michael_ref)) %>%
  arrange(desc(N_michael)) 
```

Rating by episode
```{r}
p = office_ratings %>%
  ggplot(aes(x = air_date, y = imdb_rating)) + 
  geom_point() + 
  geom_smooth(se = FALSE)

plotly::ggplotly(p)
```

